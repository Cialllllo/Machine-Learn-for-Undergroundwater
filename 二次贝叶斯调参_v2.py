# -*- coding: utf-8 -*-
"""
æ–‡ä»¶å: äºŒæ¬¡è´å¶æ–¯è°ƒå‚_v2.py
åŠŸèƒ½: è¯»å–ä¸Šä¸€è½®æœ€ä¼˜ XGBoost æ¨¡å‹ï¼Œç¼©å°èŒƒå›´è¿›è¡ŒäºŒæ¬¡ç²¾è°ƒ
ä½œè€…: æ¬§ä½©å¥‡ & ChatGPT
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import numpy as np
import joblib
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
import os
import xgboost as xgb
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

os.environ["OMP_NUM_THREADS"] = '6'

# ========== 1. è¯»å–ç‰¹å¾ä¸æ ‡ç­¾ ==========
Eigenvalue = pd.read_csv(r'ç‰¹å¾re.csv')
data_label = pd.read_csv(r'æ ‡ç­¾.csv')

# æ ‡ç­¾æ ‡å‡†åŒ–
y_standard = StandardScaler()
Y_scale = y_standard.fit_transform(data_label)

# åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    Eigenvalue, Y_scale, test_size=0.3, random_state=30
)

# ========== 2. è½½å…¥ä¸Šä¸€è½®æœ€ä¼˜æ¨¡å‹å‚æ•° ==========
if not os.path.exists('best_multi_xgb_re.pkl'):
    raise FileNotFoundError("æœªæ‰¾åˆ° best_multi_xgb_re.pklï¼Œè¯·å…ˆè¿è¡Œç¬¬ä¸€é˜¶æ®µè°ƒå‚è„šæœ¬ã€‚")

old_model = joblib.load('best_multi_xgb_re.pkl')
best_params_old = old_model.get_params()
print("âœ… å·²åŠ è½½ç¬¬ä¸€è½®æœ€ä¼˜æ¨¡å‹å‚æ•°ï¼š")
for k, v in best_params_old.items():
    if "estimator__" in k:
        print(f"{k}: {v}")

# ========== 3. å®šä¹‰ç¬¬äºŒé˜¶æ®µæ›´çª„çš„æœç´¢ç©ºé—´ ==========
# ä»¥ç¬¬ä¸€è½®æœ€ä¼˜å‚æ•°ä¸ºä¸­å¿ƒï¼Œç¼©å°èŒƒå›´
search_spaces_v2 = {
    'estimator__learning_rate': Real(
        max(0.02, best_params_old.get('estimator__learning_rate', 0.05) * 0.5),
        min(0.08, best_params_old.get('estimator__learning_rate', 0.05) * 1.5),
        prior='log-uniform'
    ),
    'estimator__n_estimators': Integer(1500, 2500),
    'estimator__max_depth': Integer(4, 6),
    'estimator__min_child_weight': Integer(1, 6),
    'estimator__subsample': Real(0.7, 0.9),
    'estimator__colsample_bytree': Real(0.45, 0.7),
    'estimator__reg_alpha': Real(0.0, 8.0),
    'estimator__reg_lambda': Real(5.0, 15.0),
    'estimator__colsample_bylevel': Real(0.8, 1.0),
    'estimator__colsample_bynode': Real(0.8, 1.0),
    'estimator__grow_policy': Categorical(['depthwise', 'lossguide']),
}

# ========== 4. åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ ==========
base_model = xgb.XGBRegressor(
    tree_method='hist',
    device='cuda',
    random_state=42,
    verbosity=0
)

multi_model = MultiOutputRegressor(base_model, n_jobs=-1)

# ========== 5. äºŒæ¬¡è´å¶æ–¯è°ƒå‚ ==========
opt2 = BayesSearchCV(
    estimator=multi_model,
    search_spaces=search_spaces_v2,
    n_iter=60,
    cv=5,
    scoring='r2',
    n_jobs=1,
    verbose=2,
    random_state=42
)

print("\nğŸš€ å¼€å§‹ç¬¬äºŒé˜¶æ®µè´å¶æ–¯ç²¾è°ƒ...")
opt2.fit(X_train, y_train)

# ========== 6. ä¿å­˜æ¨¡å‹ ==========
joblib.dump(opt2.best_estimator_, 'best_multi_xgb_re_v2.pkl')

# ========== 7. ç»“æœè¯„ä¼° ==========
y_pred = opt2.predict(X_test)
r2_scores = [r2_score(y_test[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]
mean_r2 = r2_score(y_test, y_pred)
cv_r2 = cross_val_score(opt2.best_estimator_, X_train, y_train, cv=10).mean()

print("\n========== ç²¾è°ƒç»“æœ ==========")
print(f"å¹³å‡ RÂ²: {mean_r2:.4f}")
print(f"10 æŠ˜äº¤å‰éªŒè¯ RÂ²: {cv_r2:.4f}")
print("æ¯ä¸ªè¾“å‡ºç»´åº¦ RÂ²:", r2_scores)
print("æœ€ä¼˜å‚æ•°:\n", opt2.best_params_)

# ========== 8. å‚æ•°å¯¹æ¯”æŠ¥å‘Š ==========
print("\n========== å‚æ•°å¯¹æ¯”æŠ¥å‘Š ==========")
for key, new_val in opt2.best_params_.items():
    old_val = best_params_old.get(key)
    if old_val != new_val:
        print(f"{key}: {old_val} â†’ {new_val}")
    else:
        print(f"{key}: ä¿æŒä¸å˜ ({old_val})")

print("\nâœ… äºŒæ¬¡è´å¶æ–¯è°ƒå‚å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ä¸º best_multi_xgb_re_v2.pkl")
